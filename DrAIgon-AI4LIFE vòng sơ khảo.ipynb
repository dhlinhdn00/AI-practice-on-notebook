{"cells":[{"cell_type":"markdown","source":["# Set up lib and envs"],"metadata":{"id":"tIdfMyegMOrV"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"pP0yAiIAJm_c","executionInfo":{"status":"ok","timestamp":1715305201979,"user_tz":-420,"elapsed":108190,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"outputs":[],"source":["%%capture\n","!pip install super-gradients"]},{"cell_type":"code","source":["import torch\n","import os\n","import pathlib\n","import re\n","from imutils import paths\n","import numpy as np\n","import csv\n","import json\n","import matplotlib.pyplot as plt\n","import cv2\n","import time\n","import PIL"],"metadata":{"id":"MnTWGLfetsb3","executionInfo":{"status":"ok","timestamp":1715305205770,"user_tz":-420,"elapsed":3809,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"8UvheVOR8-F6","executionInfo":{"status":"ok","timestamp":1715305205771,"user_tz":-420,"elapsed":21,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"outputs":[],"source":["import logging\n","logging.getLogger().setLevel(logging.CRITICAL)"]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"xnttWbk_mEbT","executionInfo":{"status":"ok","timestamp":1715305205771,"user_tz":-420,"elapsed":20,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"XxGxMuUvhh2y","executionInfo":{"status":"ok","timestamp":1715305231873,"user_tz":-420,"elapsed":26120,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"58cea29f-35c3-4867-addf-702e8684c4c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"TbDPi-8VNbFd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715305248477,"user_tz":-420,"elapsed":16622,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}},"outputId":"70631db0-1f1e-4cde-9fd2-f9b202995baa"},"outputs":[{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:40:31] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n"]},{"output_type":"stream","name":"stdout","text":["The console stream is logged into /root/sg_logs/console.log\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:40:37] INFO - utils.py - NumExpr defaulting to 8 threads.\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: boto3 required but not found\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: deprecated required but not found\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: coverage required but not found\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: sphinx-rtd-theme required but not found\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: torchmetrics required but not found\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: hydra-core required but not found\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: omegaconf required but not found\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: onnxruntime required but not found\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: onnx required but not found\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: pillow==9.4.0 does not satisfy requirement pillow>=10.2.0\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: einops required but not found\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: treelib required but not found\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: stringcase required but not found\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: rapidfuzz required but not found\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: json-tricks required but not found\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: onnxsim required but not found\u001b[0m\n","[2024-05-10 01:40:40] WARNING - env_sanity_check.py - \u001b[31mFailed to verify installed packages: data-gradients required but not found\u001b[0m\n","[2024-05-10 01:40:42] WARNING - checkpoint_utils.py - :warning: The pre-trained models provided by SuperGradients may have their own licenses or terms and conditions derived from the dataset used for pre-training.\n"," It is your responsibility to determine whether you have permission to use the models for your use case.\n"," The model you have requested was pre-trained on the coco_pose dataset, published under the following terms: https://cocodataset.org/#termsofuse\n","[2024-05-10 01:40:42] INFO - checkpoint_utils.py - License Notification: YOLO-NAS-POSE pre-trained weights are subjected to the specific license terms and conditions detailed in \n","https://github.com/Deci-AI/super-gradients/blob/master/LICENSE.YOLONAS-POSE.md\n","By downloading the pre-trained weight files you agree to comply with these terms.\n","Downloading: \"https://sghub.deci.ai/models/yolo_nas_pose_l_coco_pose.pth\" to /root/.cache/torch/hub/checkpoints/yolo_nas_pose_l_coco_pose.pth\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 304M/304M [00:04<00:00, 65.8MB/s]\n","[2024-05-10 01:40:47] INFO - checkpoint_utils.py - Successfully loaded pretrained weights for architecture yolo_nas_pose_l\n"]}],"source":["from super_gradients.training import models\n","from super_gradients.common.object_names import Models\n","\n","yolo_nas_pose = models.get(\"yolo_nas_pose_l\", pretrained_weights=\"coco_pose\").cuda()"]},{"cell_type":"markdown","metadata":{"id":"9Jzti2WkfYko"},"source":["# ü©ª Display only the skeleton\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"9o-6l9v-fY4X","executionInfo":{"status":"ok","timestamp":1715305248477,"user_tz":-420,"elapsed":51,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"outputs":[],"source":["from super_gradients.training.utils.visualization.detection import draw_bbox\n","from super_gradients.training.utils.visualization.pose_estimation import PoseVisualization\n","\n","def process_single_image(image_prediction):\n","    \"\"\"\n","    Process a single image prediction to visualize the pose estimation results on a blank background.\n","\n","    Parameters:\n","    - image_prediction : object\n","        An instance containing the image and its associated pose prediction data.\n","\n","    Returns:\n","    - np.ndarray\n","        An image with the pose skeleton drawn.\n","    \"\"\"\n","\n","    image = image_prediction.image\n","    pose_data = image_prediction.prediction\n","\n","    blank_image = np.zeros_like(image)  # for a black background\n","\n","    skeleton_image = PoseVisualization.draw_poses(\n","        image=blank_image,\n","        poses=pose_data.poses,\n","        boxes=pose_data.bboxes_xyxy,\n","        scores=pose_data.scores,\n","        is_crowd=None,\n","        edge_links=pose_data.edge_links,\n","        edge_colors=pose_data.edge_colors,\n","        keypoint_colors=pose_data.keypoint_colors,\n","        joint_thickness=2,\n","        box_thickness=2,\n","        keypoint_radius=5\n","    )\n","    return skeleton_image, pose_data, image"]},{"cell_type":"code","source":["def create_video_from_frames(frames, output_filename='output_video.mp4', fps=30.0):\n","    \"\"\"\n","    Create an mp4 video from a list of image frames.\n","\n","    Parameters:\n","    - frames : list of np.ndarray\n","        List of image frames represented as numpy arrays.\n","    - output_filename : str, optional\n","        Name of the output video file.\n","    - fps : float, optional\n","        Frames per second for the output video.\n","\n","    Returns:\n","    - None\n","    \"\"\"\n","\n","    # Determine the width and height from the first frame\n","    frame_height, frame_width, layers = frames[0].shape\n","\n","    # Define the codec for .mp4 format\n","    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n","    out = cv2.VideoWriter(output_filename, fourcc, fps, (frame_width, frame_height))\n","\n","    # Write each frame to the video\n","    for frame in frames:\n","        out.write(frame)\n","\n","    # Close and release everything\n","    out.release()\n","    cv2.destroyAllWindows()"],"metadata":{"id":"Tu6iTV5dI_a5","executionInfo":{"status":"ok","timestamp":1715305248477,"user_tz":-420,"elapsed":49,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dE7gUJA4tS0k"},"source":["# üìΩÔ∏è Data Processing"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"F3g9bX6u92aA","executionInfo":{"status":"ok","timestamp":1715305248478,"user_tz":-420,"elapsed":48,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"outputs":[],"source":["def normalize_data(poses, bboxes, frame_width, frame_height):\n","    normalized_poses = np.copy(poses)\n","    normalized_bboxes = np.copy(bboxes)\n","\n","    normalized_poses[:, :, 0] /= frame_width\n","    normalized_poses[:, :, 1] /= frame_height\n","\n","    normalized_bboxes[:, [0, 2]] /= frame_width\n","    normalized_bboxes[:, [1, 3]] /= frame_height\n","\n","    return normalized_poses, normalized_bboxes"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"7lWt4pPR8bRg","executionInfo":{"status":"ok","timestamp":1715305248478,"user_tz":-420,"elapsed":47,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"outputs":[],"source":["def select_primary_pose(poses, bboxes, frame_width, frame_height):\n","    center_of_frame = np.array([frame_width / 2, frame_height / 2])\n","    highest_score = -1\n","    primary_pose = None\n","    primary_bbox = None\n","\n","    for pose, bbox in zip(poses, bboxes):\n","        bbox_center = np.array([(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2])\n","        distance_to_center = np.linalg.norm(center_of_frame - bbox_center)\n","        bbox_area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n","\n","        score = bbox_area / (distance_to_center + 1)\n","\n","        if score > highest_score:\n","            highest_score = score\n","            primary_pose = pose\n","            primary_bbox = bbox\n","\n","    return primary_pose, primary_bbox"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"u3NawU0wGvW7","executionInfo":{"status":"ok","timestamp":1715305248478,"user_tz":-420,"elapsed":46,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"outputs":[],"source":["def extract_pose_information_per_frame(video_path, conf_threshold=0.4):\n","    # Load model\n","    # yolo_nas_pose = models.get(\"yolo_nas_pose_l\", pretrained_weights=\"coco_pose\").cuda()\n","    result = yolo_nas_pose.to('cuda').predict(video_path, conf=conf_threshold)\n","\n","    frame_data_list = []\n","\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        print(\"Could not open video file\")\n","    ret, frame = cap.read()\n","    frame_height, frame_width = frame.shape[:2]\n","    cap.release()\n","\n","    for image_prediction in result._images_prediction_gen:\n","        poses = image_prediction.prediction.poses\n","        bboxes_xyxy = image_prediction.prediction.bboxes_xyxy\n","        edge_links = image_prediction.prediction.edge_links\n","\n","        normalized_poses, normalized_bboxes = normalize_data(poses, bboxes_xyxy, frame_width, frame_height)\n","\n","        primary_pose, primary_bbox = select_primary_pose(normalized_poses, normalized_bboxes, 1, 1)  #\n","\n","        if primary_pose is not None:\n","            frame_data = {\n","                'poses': np.array([primary_pose]),\n","                'bboxes_xyxy': np.array([primary_bbox]),\n","                'edge_links': edge_links\n","            }\n","            frame_data_list.append(frame_data)\n","    print(f\"Finish processing the video {video_path}\")\n","    return frame_data_list\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"5Mc8D0NHvMaM","executionInfo":{"status":"ok","timestamp":1715305248479,"user_tz":-420,"elapsed":46,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"outputs":[],"source":["def remove_outlier_frames(frame_data_list, area_threshold=0.1, distance_threshold=0.1):\n","    filtered_frames = []\n","    prev_area = None\n","    prev_center = np.array([0.5, 0.5])\n","\n","    for frame_data in frame_data_list:\n","        if 'bboxes_xyxy' not in frame_data or len(frame_data['bboxes_xyxy']) == 0:\n","            continue\n","\n","        bbox = frame_data['bboxes_xyxy'][0]\n","        area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n","        center = np.array([(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2])\n","\n","        if prev_area is not None:\n","            area_change = abs(area - prev_area)\n","            center_distance = np.linalg.norm(center - prev_center)\n","\n","            # Check\n","            if area_change > area_threshold or center_distance > distance_threshold:\n","                continue\n","\n","        prev_area = area\n","        prev_center = center\n","        filtered_frames.append(frame_data)\n","\n","    return filtered_frames\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"GMDhgTmg92cb","executionInfo":{"status":"ok","timestamp":1715305248479,"user_tz":-420,"elapsed":45,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"outputs":[],"source":["def processing_for_pose(pose_data_list, save_format=\"npy\", poselist_filename='/content/drive/MyDrive/sample', video_filename=None, frame_size=(640,480), fps=30.0):\n","    if video_filename is not None:\n","        fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n","        out = cv2.VideoWriter(video_filename, fourcc, fps, frame_size)\n","\n","    frame_width, frame_height = frame_size\n","    pose_data_list = remove_outlier_frames(pose_data_list) # Remove outlier step\n","    only_pose_list = []\n","    for frame_data in pose_data_list:\n","        if video_filename is not None:\n","            frame = np.zeros((frame_height, frame_width, 3), np.uint8)\n","        for pose in frame_data['poses']:\n","            if video_filename is not None:\n","                for link in frame_data['edge_links']:\n","                    start_point = (int(pose[link[0], 0] * frame_width), int(pose[link[0], 1] * frame_height))\n","                    end_point = (int(pose[link[1], 0] * frame_width), int(pose[link[1], 1] * frame_height))\n","                    color = (0, 255, 0)  # Skeleton Color\n","                    cv2.line(frame, start_point, end_point, color, 2)\n","        only_pose_list.append(pose)\n","        if video_filename is not None:\n","            out.write(frame)\n","\n","    if video_filename is not None:\n","        out.release()\n","\n","    if save_format == 'npy':\n","        np.save(poselist_filename + '.npy', np.array(only_pose_list))\n","    elif save_format == 'csv':\n","        with open(poselist_filename + '.csv', mode='w', newline='') as file:\n","            writer = csv.writer(file)\n","            for pose in only_pose_list:\n","                # Flatten pose array\n","                flattened_pose = pose.flatten()\n","                writer.writerow(flattened_pose)\n","    elif save_format == \"json\":\n","        pose_list_as_lists = [pose.tolist() for pose in only_pose_list]\n","        with open(poselist_filename + '.json', 'w') as file:\n","            json.dump(pose_list_as_lists, file)\n","    else:\n","        print(\"Save format is invalid!\")\n","\n","    return only_pose_list"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"aLvU8VB192n3","executionInfo":{"status":"ok","timestamp":1715305248479,"user_tz":-420,"elapsed":44,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"outputs":[],"source":["def data_processing(input_dir, list_data_dir, video_data_dir=None, frame_size=(640,480), fps=30.0):\n","    os.makedirs(list_data_dir, exist_ok=True)\n","    if video_data_dir is not None:\n","        os.makedirs(video_data_dir, exist_ok=True)\n","\n","    for category in os.listdir(input_dir):\n","        print(f\"***Processing: {category} ***\")\n","        start_time = time.time()\n","\n","        category_path = os.path.join(input_dir, category)\n","        list_category_path = os.path.join(list_data_dir, category)\n","\n","        os.makedirs(list_category_path, exist_ok=True)\n","        if video_data_dir is not None:\n","            video_category_path = os.path.join(video_data_dir, category)\n","            os.makedirs(video_category_path, exist_ok=True)\n","\n","        video_count = 0\n","\n","        if os.path.isdir(category_path):\n","            for video_file in os.listdir(category_path):\n","                video_path = os.path.join(category_path, video_file)\n","                base_filename = os.path.splitext(video_file)[0]\n","                poselist_path = os.path.join(list_category_path, base_filename)\n","                video_output_path = None if video_data_dir is None else os.path.join(video_category_path, base_filename + '.mp4')\n","\n","                if video_path.lower().endswith(('.mp4', '.mov', '.MOV')):\n","                    pose_data = extract_pose_information_per_frame(video_path)\n","                    only_pose_list = processing_for_pose(pose_data,\n","                                                         save_format=\"npy\",\n","                                                         poselist_filename=poselist_path,\n","                                                         video_filename=video_output_path,\n","                                                         frame_size=frame_size,\n","                                                         fps=fps)\n","                    video_count += 1\n","\n","        elapsed_time = time.time() - start_time\n","        print(f\"Processed {video_count} videos in '{category}' in {elapsed_time:.2f} seconds.\")\n","\n","        if video_data_dir is not None:\n","            print(f\"Total videos processed in '{video_data_dir}/{category}': {video_count}\")"]},{"cell_type":"markdown","source":["# Apply for dataset"],"metadata":{"id":"ANYnz6WVTxaQ"}},{"cell_type":"code","source":["# For data v√≤ng lo·∫°i\n","input_dir = '/content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i'\n","list_data_dir = '/content/drive/MyDrive/DrAIgon/list_processing_data'"],"metadata":{"id":"E-sk2m3qT8ze","executionInfo":{"status":"ok","timestamp":1715305248480,"user_tz":-420,"elapsed":43,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":143030,"status":"error","timestamp":1711882830053,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"},"user_tz":-420},"id":"ltM_i6AE6jQs","outputId":"f919e7ff-b493-4deb-a5dc-674e0750dd4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["***Processing: tricep Pushdown ***\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:58:08] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_10.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:58:12] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_13.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:58:16] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_14.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:58:19] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_16.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:58:23] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_11.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:58:29] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_15.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:58:33] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_1.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:58:38] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_12.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:58:44] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_2.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:58:49] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_17.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:58:53] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_20.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:58:57] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_21.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:59:01] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_19.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:59:07] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_23.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:59:11] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_24.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:59:16] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_22.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:59:20] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_18.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:59:26] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_26.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:59:31] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_33.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:59:35] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_3.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:59:40] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_31.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:59:45] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_32.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:59:50] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_25.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 10:59:54] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_28.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 11:00:00] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_30.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 11:00:06] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_29.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 11:00:11] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_27.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 11:00:15] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_4.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 11:00:19] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_34.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 11:00:24] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Data - VoÃÄng loaÃ£i/tricep Pushdown/tricep pushdown_38.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-03-31 11:00:29] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-4de53fa491a7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_data_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-e297a3c1ff04>\u001b[0m in \u001b[0;36mdata_processing\u001b[0;34m(input_dir, list_data_dir, video_data_dir, frame_size, fps)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mvideo_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mp4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.mov'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.MOV'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mpose_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_pose_information_per_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                     only_pose_list = processing_for_pose(pose_data,\n\u001b[1;32m     30\u001b[0m                                                          \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"npy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-14f7150ebb99>\u001b[0m in \u001b[0;36mextract_pose_information_per_frame\u001b[0;34m(video_path, conf_threshold)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_prediction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_images_prediction_gen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mposes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbboxes_xyxy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbboxes_xyxy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/super_gradients/training/pipelines/pipelines.py\u001b[0m in \u001b[0;36m_generate_prediction_result\u001b[0;34m(self, images, batch_size)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_images\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_prediction_result_single_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_generate_prediction_result_single_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mImagePrediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/super_gradients/training/pipelines/pipelines.py\u001b[0m in \u001b[0;36m_generate_prediction_result_single_batch\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuse_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fuse_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_model_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/super_gradients/training/models/detection_models/customizable_detector.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_initialize_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_eps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_momentum\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace_act\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/super_gradients/training/models/pose_estimation_models/yolo_nas_pose/yolo_nas_pose_ndfl_heads.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, feats)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0manchor_points_inference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0manchor_points_inference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mpred_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_score_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/super_gradients/training/models/pose_estimation_models/yolo_nas_pose/yolo_nas_pose_ndfl_heads.py\u001b[0m in \u001b[0;36m_generate_anchors\u001b[0;34m(self, feats, dtype, device)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0manchor_points\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manchor_points\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0mstride_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstride_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0manchor_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["data_processing(input_dir, list_data_dir, video_data_dir=None)"]},{"cell_type":"code","source":["# For test v√≤ng lo·∫°i\n","input_dir_test = '/content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Test - VoÃÄng loaÃ£i - 2'\n","list_data_dir_test = '/content/drive/MyDrive/DrAIgon/list_processing_data_testvongloai'"],"metadata":{"id":"_zwHaW_bf4UZ","executionInfo":{"status":"ok","timestamp":1715305248480,"user_tz":-420,"elapsed":43,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["data_processing(input_dir_test, list_data_dir_test, video_data_dir_test=None)"],"metadata":{"id":"36kWDE3fgTyP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Util"],"metadata":{"id":"6BZhp_UbLEUj"}},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","\n","class GymPoseDataset(Dataset):\n","    def __init__(self, list_data_dir, transform=None, augment=False, fixed_length=100):\n","        self.list_data_dir = list_data_dir\n","        self.transform = transform\n","        self.augment = augment\n","        self.fixed_length = fixed_length\n","        self.data_info, self.class_to_index = self.get_data_info_and_class_mapping()\n","\n","    def get_data_info_and_class_mapping(self):\n","        data_info = []\n","        class_names = set()\n","        for class_dir in os.listdir(self.list_data_dir):\n","            class_dir_path = os.path.join(self.list_data_dir, class_dir)\n","            if os.path.isdir(class_dir_path):\n","                for file_name in os.listdir(class_dir_path):\n","                    file_path = os.path.join(class_dir_path, file_name)\n","                    data_info.append((file_path, class_dir))\n","                    class_names.add(class_dir)\n","        class_to_index = {class_name: i for i, class_name in enumerate(sorted(class_names))}\n","        return data_info, class_to_index\n","\n","\n","    def __len__(self):\n","        return len(self.data_info)\n","\n","    def time_warp(self, data, warp_factor=0.2):\n","        sequence_length = data.shape[0]\n","        new_length = int(sequence_length * warp_factor)\n","\n","        warped_data = np.zeros((new_length, data.shape[1], data.shape[2]))\n","        for i in range(data.shape[1]):\n","            for j in range(data.shape[2]):\n","                warped_data[:, i, j] = np.interp(np.linspace(0, sequence_length, new_length), np.arange(sequence_length), data[:, i, j])\n","\n","        return warped_data\n","\n","    def __getitem__(self, idx):\n","        file_path, class_name = self.data_info[idx]\n","        data = np.load(file_path)\n","\n","        if self.augment:\n","            data = self.time_warp(data)\n","\n","        if data.shape[0] < self.fixed_length:\n","            padding = np.zeros((self.fixed_length - data.shape[0], data.shape[1], data.shape[2]))\n","            data = np.concatenate((data, padding), axis=0)\n","        elif data.shape[0] > self.fixed_length:\n","            data = data[:self.fixed_length, :, :]\n","\n","        data = data[:, :, :2].reshape(data.shape[0], -1)\n","        data = torch.as_tensor(data, dtype=torch.float)\n","        label = torch.tensor(self.class_to_index[class_name], dtype=torch.long)\n","\n","        return data, label"],"metadata":{"id":"CrQqDRbIC2NF","executionInfo":{"status":"ok","timestamp":1715305248480,"user_tz":-420,"elapsed":42,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n","\n","def pad_collate(batch):\n","    (xx, yy) = zip(*batch)\n","    x_lens = torch.tensor([x.size(0) for x in xx], dtype=torch.long)\n","\n","    xx_pad = pad_sequence(xx, batch_first=True, padding_value=0)\n","    yy_stack = torch.stack(yy, dim=0)\n","    return xx_pad, yy_stack, x_lens"],"metadata":{"id":"n3yaAPFxC7Ll","executionInfo":{"status":"ok","timestamp":1715305248480,"user_tz":-420,"elapsed":40,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def create_dataloaders(dataset, batch_size=32, val_split=0.1, test_split=0.1):\n","    dataset_size = len(dataset)\n","    test_size = int(test_split * dataset_size)\n","    val_size = int(val_split * dataset_size)\n","    train_size = dataset_size - val_size - test_size\n","    train_dataset, val_test_dataset = random_split(dataset, [train_size, val_size + test_size])\n","    val_dataset, test_dataset = random_split(val_test_dataset, [val_size, test_size])\n","    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=pad_collate)\n","    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=pad_collate)\n","    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=pad_collate)\n","    return train_loader, val_loader, test_loader"],"metadata":{"id":"X2NrV6Tw2yhq","executionInfo":{"status":"ok","timestamp":1715305248481,"user_tz":-420,"elapsed":40,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def create_test_dataloader(dataset, batch_size=32):\n","    test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate)\n","    return test_loader"],"metadata":{"id":"8I64PN2s20JZ","executionInfo":{"status":"ok","timestamp":1715305248481,"user_tz":-420,"elapsed":39,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","def evaluate_model(model, loader, criterion, device, mode = 2):\n","    # mode = 1 for LSTM\n","    # mode = 2 for ST-GCN\n","    model.eval()\n","    running_loss, total_samples = 0.0, 0\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for data, labels, lengths in loader:\n","            data, labels, lengths = data.to(device), labels.to(device), lengths.to(device)\n","            if mode == 1:\n","                outputs = model(data, lengths)\n","            elif mode == 2:\n","                outputs = model(data)\n","            loss = criterion(outputs, labels)\n","            _, preds = torch.max(outputs, 1)\n","\n","            running_loss += loss.item() * data.size(0)\n","            total_samples += labels.size(0)\n","\n","            all_preds.extend(preds.view(-1).cpu().numpy())\n","            all_labels.extend(labels.view(-1).cpu().numpy())\n","\n","    epoch_loss = running_loss / total_samples\n","    accuracy = 100 * (np.array(all_preds) == np.array(all_labels)).mean()\n","    f1 = f1_score(all_labels, all_preds, average='weighted')\n","    precision = precision_score(all_labels, all_preds, average='weighted')\n","    recall = recall_score(all_labels, all_preds, average='weighted')\n","    conf_matrix = confusion_matrix(all_labels, all_preds)\n","\n","    return epoch_loss, accuracy, f1, precision, recall, conf_matrix"],"metadata":{"id":"3cHX96v5DB8m","executionInfo":{"status":"ok","timestamp":1715305248481,"user_tz":-420,"elapsed":39,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def plot_training_history(train_losses, train_accuracies, val_losses, val_accuracies):\n","    plt.figure(figsize=(12, 6))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_losses, label='Train Loss')\n","    plt.plot(val_losses, label='Validation Loss')\n","    plt.title('Loss History')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.subplot(1, 2, 2)\n","    plt.plot(train_accuracies, label='Train Accuracy')\n","    plt.plot(val_accuracies, label='Validation Accuracy')\n","    plt.title('Accuracy History')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy (%)')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"GCEpxxnl_bGm","executionInfo":{"status":"ok","timestamp":1715305248481,"user_tz":-420,"elapsed":38,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","def train_model(model, train_loader, val_loader, device, num_epochs=80, weight_path='best_model_weights.pth', mode = 2):\n","    model.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1)\n","\n","    best_val_loss = 99\n","\n","    train_losses, train_accuracies = [], []\n","    val_losses, val_accuracies = [], []\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss, running_corrects, total_samples = 0.0, 0, 0\n","        for data, labels, lengths in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n","            data, labels, lengths = data.to(device), labels.to(device), lengths.to(device)\n","            optimizer.zero_grad()\n","            if mode == 1:\n","                outputs = model(data, lengths)\n","            elif mode == 2:\n","                outputs = model(data)\n","            loss = criterion(outputs, labels)\n","            _, preds = torch.max(outputs, 1)\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","            optimizer.step()\n","            # scheduler.step()\n","\n","            running_loss += loss.item() * data.size(0)\n","            running_corrects += torch.sum(preds == labels.data)\n","            total_samples += labels.size(0)\n","\n","        epoch_loss = running_loss / total_samples\n","        epoch_acc = running_corrects.double()*100 / total_samples\n","\n","        train_losses.append(epoch_loss)\n","        train_accuracies.append(epoch_acc.item())\n","\n","        val_loss, val_accuracy, _, _, _, _ = evaluate_model(model, val_loader, criterion, device, mode)\n","        scheduler.step(val_loss)\n","        val_losses.append(val_loss)\n","        val_accuracies.append(val_accuracy)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            torch.save(model.state_dict(), weight_path)\n","\n","        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n","\n","    plot_training_history(train_losses, train_accuracies, val_losses, val_accuracies)"],"metadata":{"id":"A8zOwYuUC-u7","executionInfo":{"status":"ok","timestamp":1715305248482,"user_tz":-420,"elapsed":38,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def test_model(model, test_loader, device, weight_path='best_model_weights.pth', mode = 2):\n","    model.load_state_dict(torch.load(weight_path))\n","    model.to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    test_loss, test_accuracy, test_f1, test_precision, test_recall, test_conf_matrix = evaluate_model(model, test_loader, criterion, device, mode=mode)\n","\n","    print(f'------------Testing on the best weight------------')\n","    print(f'Test Loss: {test_loss:.4f}, Test Acc: {test_accuracy:.2f}%')\n","    print(f'F1 Score: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}')\n","    print('Confusion Matrix:\\n', test_conf_matrix)"],"metadata":{"id":"1iodclEoE2Ve","executionInfo":{"status":"ok","timestamp":1715305248483,"user_tz":-420,"elapsed":38,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def single_inference(model, video_filename, device, class_to_index, fixed_length=100, weight_path='best_model_weights.pth'):\n","    pose_data = extract_pose_information_per_frame(video_filename)\n","    pose_list = processing_for_pose(pose_data)\n","    input_data = np.array(pose_list)\n","\n","    model.load_state_dict(torch.load(weight_path))\n","    model.to(device)\n","    model.eval()\n","\n","    if input_data.shape[0] < fixed_length:\n","        padding = np.zeros((fixed_length - input_data.shape[0], input_data.shape[1], input_data.shape[2]))\n","        input_data = np.concatenate((input_data, padding), axis=0)\n","    elif input_data.shape[0] > fixed_length:\n","        input_data = input_data[:fixed_length, :, :]\n","\n","    input_data = input_data[:, :, :2].reshape(input_data.shape[0], -1)\n","    input_data = torch.as_tensor(input_data, dtype=torch.float).unsqueeze(0)\n","    input_data = input_data.to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(input_data)\n","        _, preds = torch.max(outputs, 1)\n","\n","    index_to_class = {v: k for k, v in class_to_index.items()}\n","    predicted_class_name = index_to_class[preds.item()]\n","\n","    return predicted_class_name"],"metadata":{"id":"G4dbFcbk3IM9","executionInfo":{"status":"ok","timestamp":1715305248483,"user_tz":-420,"elapsed":37,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["# Training: ST-GCN"],"metadata":{"id":"8D0PUSev8-ZS"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"wsZF2RULL47N","executionInfo":{"status":"ok","timestamp":1715305248483,"user_tz":-420,"elapsed":36,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["class Graph():\n","  def __init__(self, hop_size):\n","      self.num_node = 17\n","      self.get_edge()\n","      self.hop_size = hop_size\n","      self.hop_dis = self.get_hop_distance(self.num_node, self.edge, hop_size=hop_size)\n","      self.get_adjacency()\n","\n","  def __str__(self):\n","      return self.A\n","\n","  def get_edge(self):\n","      self_link = [(i, i) for i in range(self.num_node)]\n","      neighbor_link = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 4), (3, 5), (4, 6),\n","                        (5, 6), (5, 7), (5, 11), (6, 8), (6, 12), (7, 9), (8, 10),\n","                        (11, 12), (11, 13), (12, 14), (13, 15), (14, 16)]\n","      self.edge = self_link + neighbor_link\n","\n","  def get_adjacency(self):\n","      valid_hop = range(0, self.hop_size + 1, 1)\n","      adjacency = np.zeros((self.num_node, self.num_node))\n","      for hop in valid_hop:\n","          adjacency[self.hop_dis == hop] = 1\n","      normalize_adjacency = self.normalize_digraph(adjacency)\n","      A = np.zeros((len(valid_hop), self.num_node, self.num_node))\n","      for i, hop in enumerate(valid_hop):\n","          A[i][self.hop_dis == hop] = normalize_adjacency[self.hop_dis == hop]\n","      self.A = A\n","\n","  def get_hop_distance(self, num_node, edge, hop_size):\n","      A = np.zeros((num_node, num_node))\n","      for i, j in edge:\n","          A[j, i] = 1\n","          A[i, j] = 1\n","      hop_dis = np.zeros((num_node, num_node)) + np.inf\n","      transfer_mat = [np.linalg.matrix_power(A, d) for d in range(hop_size + 1)]\n","      arrive_mat = (np.stack(transfer_mat) > 0)\n","      for d in range(hop_size, -1, -1):\n","          hop_dis[arrive_mat[d]] = d\n","      return hop_dis\n","\n","  def normalize_digraph(self, A):\n","      Dl = np.sum(A, 0)\n","      num_node = A.shape[0]\n","      Dn = np.zeros((num_node, num_node))\n","      for i in range(num_node):\n","          if Dl[i] > 0:\n","              Dn[i, i] = Dl[i]**(-1)\n","      DAD = np.dot(A, Dn)\n","      return DAD"],"metadata":{"id":"hGrStZE27znD","executionInfo":{"status":"ok","timestamp":1715305248483,"user_tz":-420,"elapsed":35,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["class SpatialGraphConvolution(nn.Module):\n","  def __init__(self, in_channels, out_channels, s_kernel_size):\n","    super().__init__()\n","    self.s_kernel_size = s_kernel_size\n","    self.conv = nn.Conv2d(in_channels=in_channels,\n","                          out_channels=out_channels * s_kernel_size,\n","                          kernel_size=1)\n","\n","  def forward(self, x, A):\n","    x = self.conv(x)\n","    n, kc, t, v = x.size()\n","    x = x.view(n, self.s_kernel_size, kc//self.s_kernel_size, t, v)\n","    x = torch.einsum('nkctv,kvw->nctw', (x, A))\n","    return x.contiguous()"],"metadata":{"id":"aIJzXaZ_72sr","executionInfo":{"status":"ok","timestamp":1715305248484,"user_tz":-420,"elapsed":35,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["class STGC_block(nn.Module):\n","  def __init__(self, in_channels, out_channels, stride, t_kernel_size, A_size, dropout=0.5):\n","    super().__init__()\n","    self.sgc = SpatialGraphConvolution(in_channels=in_channels,\n","                                       out_channels=out_channels,\n","                                       s_kernel_size=A_size[0])\n","\n","    self.M = nn.Parameter(torch.ones(A_size))\n","\n","    self.tgc = nn.Sequential(nn.BatchNorm2d(out_channels),\n","                            nn.ReLU(),\n","                            nn.Dropout(dropout),\n","                            nn.Conv2d(out_channels,\n","                                      out_channels,\n","                                      (t_kernel_size, 1),\n","                                      (stride, 1),\n","                                      ((t_kernel_size - 1) // 2, 0)),\n","                            nn.BatchNorm2d(out_channels),\n","                            nn.ReLU())\n","\n","  def forward(self, x, A):\n","    x = self.tgc(self.sgc(x, A * self.M))\n","    return x"],"metadata":{"id":"JSxBWIXL75w4","executionInfo":{"status":"ok","timestamp":1715305248484,"user_tz":-420,"elapsed":34,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["class ST_GCN(nn.Module):\n","    def __init__(self, num_classes=22, num_joints=17, sequence_length=20, t_kernel_size=9, hop_size=2):\n","        super().__init__()\n","        in_channels = 2  # (x,y) not confidences\n","\n","        graph = Graph(hop_size)\n","        A = torch.tensor(graph.A, dtype=torch.float32, requires_grad=False)\n","        self.register_buffer('A', A)\n","\n","        self.stgc_blocks = nn.ModuleList([\n","            STGC_block(in_channels, 64, 1, t_kernel_size, A.shape),\n","            STGC_block(64, 128, 2, t_kernel_size, A.shape),\n","            STGC_block(128, 256, 2, t_kernel_size, A.shape),\n","            STGC_block(256, 256, 1, t_kernel_size, A.shape),\n","            STGC_block(256, 256, 1, t_kernel_size, A.shape)\n","        ])\n","\n","        self.fc = nn.Conv2d(256, num_classes, kernel_size=1)\n","\n","    def forward(self, x):\n","        N, T, VC = x.size()\n","        C = 2\n","        V = VC // C\n","        x = x.view(N, T, V, C).permute(0, 3, 1, 2)  # [N, C, T, V]\n","\n","        for stgc in self.stgc_blocks:\n","            x = stgc(x, self.A)\n","\n","        x = F.avg_pool2d(x, x.size()[2:])\n","        x = x.view(N, -1, 1, 1)\n","        x = self.fc(x)\n","        x = x.view(x.size(0), -1)\n","        return x"],"metadata":{"id":"0Vg6vTMB9pyF","executionInfo":{"status":"ok","timestamp":1715305248484,"user_tz":-420,"elapsed":33,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# Config\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","list_data_dir = '/content/drive/MyDrive/DrAIgon/list_data_vl'\n","mode = 2 #mode 2 for ST_GCN"],"metadata":{"id":"hiDtwJ4S7K64","executionInfo":{"status":"ok","timestamp":1715305248484,"user_tz":-420,"elapsed":32,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["ST_GCN = ST_GCN().to(device)"],"metadata":{"id":"zgayQL9MHI5Y","executionInfo":{"status":"ok","timestamp":1715305248485,"user_tz":-420,"elapsed":32,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["dataset = GymPoseDataset(list_data_dir, transform=transforms.ToTensor(), augment=False)\n","train_loader, val_loader, test_loader = create_dataloaders(dataset)"],"metadata":{"id":"UnCdWn-xs13N","executionInfo":{"status":"ok","timestamp":1715305251320,"user_tz":-420,"elapsed":2867,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["train_model(ST_GCN, train_loader, val_loader, device, num_epochs=100, mode=mode)"],"metadata":{"id":"czYPTbEZ7fz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_model(ST_GCN, test_loader, device, mode=mode)"],"metadata":{"id":"PjV7B6ds7hRx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Testing_VL: ST-GCN"],"metadata":{"id":"rJ9PEF7v7kAW"}},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","input_dir_test = '/content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Test - VoÃÄng loaÃ£i - 2'\n","list_data_dir_test = '/content/drive/MyDrive/DrAIgon/list_data_tvl'\n","mode = 2"],"metadata":{"id":"yb1zeArj2XoJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_test = GymPoseDataset(list_data_dir_test, transform=transforms.ToTensor(), augment=False)\n","test_loader_t = create_test_dataloader(dataset_test)"],"metadata":{"id":"GtJeQ_Ma2guT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_model(ST_GCN, test_loader_t, device, mode=mode)"],"metadata":{"id":"9Y_C_Fhg8ORf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Using the best weight\n","test_model(ST_GCN, test_loader_t, device, weight_path='/content/drive/MyDrive/DrAIgon/final_best_weight.pth', mode=mode)"],"metadata":{"id":"qOwCPGlEDMBu","executionInfo":{"status":"ok","timestamp":1711982351008,"user_tz":-420,"elapsed":8661,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"65725d69-3513-46e5-f7fe-ed2ccefcec52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["------------Testing on the best weight------------\n","Test Loss: 2.7090, Test Acc: 51.28%\n","F1 Score: 0.4844, Precision: 0.5192, Recall: 0.5128\n","Confusion Matrix:\n"," [[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0]\n"," [0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n"," [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0]\n"," [0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"," [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n"," [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]]\n"]}]},{"cell_type":"markdown","source":["# Test for single video"],"metadata":{"id":"wE8VwFM4EfKu"}},{"cell_type":"code","source":["video_filename = '/content/drive/MyDrive/DrAIgon/AI4LIFE2024-DATA/Test - VoÃÄng loaÃ£i/barbell biceps curl_1.mp4'\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","class_to_index = dataset.class_to_index\n","weight_path = '/content/drive/MyDrive/DrAIgon/final_best_weight.pth'\n","single_inference(ST_GCN,video_filename=video_filename, device=device, class_to_index=class_to_index, weight_path=weight_path )"],"metadata":{"id":"nysE2NsA6h6U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test for batch (Th·ªÉ th·ª©c m·ªõi c·ªßa ban t·ªï ch·ª©c)"],"metadata":{"id":"Vd72H0MXE0Ue"}},{"cell_type":"code","source":["def batch_inference(folder_path, model, device, class_to_index, weight_path):\n","    video_files = [f for f in os.listdir(folder_path) if f.endswith('.mp4')]\n","    results_file = os.path.join(\"inference_results.csv\")\n","\n","    with open(results_file, mode='w', newline='') as file:\n","        writer = csv.writer(file)\n","        writer.writerow(['video', 'd·ª± ƒëo√°n'])\n","\n","        for video_file in video_files:\n","            # ground_truth = video_file.rsplit('_', 1)[0]\n","\n","            predicted_class_name = single_inference(model, os.path.join(folder_path, video_file), device, class_to_index, weight_path=weight_path)\n","\n","            writer.writerow([video_file, predicted_class_name])\n"],"metadata":{"id":"z2Nh8Had-qhL","executionInfo":{"status":"ok","timestamp":1715305256632,"user_tz":-420,"elapsed":8,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["#Config\n","folder_path = \"/content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt\"\n","model = ST_GCN\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","class_to_index = dataset.class_to_index\n","print(class_to_index)\n","weight_path = '/content/drive/MyDrive/DrAIgon/final_best_weight.pth'\n","# weight_path = 'best_model_weights.pth'"],"metadata":{"id":"z4X7lS0-_HBw","executionInfo":{"status":"ok","timestamp":1715305258056,"user_tz":-420,"elapsed":10,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["batch_inference(folder_path=folder_path, model=model, device=device, class_to_index=class_to_index, weight_path=weight_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VnChrvq5CJW6","executionInfo":{"status":"ok","timestamp":1715305804593,"user_tz":-420,"elapsed":542145,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}},"outputId":"de6fb2dd-31ac-466d-f15e-9c91fe7112ff"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:41:04] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video1.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:41:31] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video2.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:41:52] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video3.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:42:13] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video4.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:42:37] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video5.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:43:25] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video6.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:43:48] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video7.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:44:31] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video8.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:44:47] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video9.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:45:01] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video10.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:45:15] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video11.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:45:30] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video12.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:45:53] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video13.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:46:21] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video14.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:46:49] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video15.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:47:25] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video16.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:47:41] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video17t.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:47:57] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video18.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:48:25] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video19.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:48:38] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video20.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:49:30] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]},{"output_type":"stream","name":"stdout","text":["Finish processing the video /content/drive/MyDrive/DrAIgon/DuÃõÃÉ lieÃ£ÃÇu kieÃÇÃâm thuÃõÃâ voÃÄng chung keÃÇÃÅt/video21.mp4\n"]},{"output_type":"stream","name":"stderr","text":["[2024-05-10 01:49:49] INFO - pipelines.py - Fusing some of the model's layers. If this takes too much memory, you can deactivate it by setting `fuse_model=False`\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","results_file = \"inference_results.csv\"\n","results_df = pd.read_csv(results_file)\n","print(results_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nmjSm999KNgY","executionInfo":{"status":"ok","timestamp":1715305810075,"user_tz":-420,"elapsed":411,"user":{"displayName":"Ho√†i Linh ƒê√†o","userId":"06427991247119097271"}},"outputId":"9c2e6080-f148-422f-99b1-84d56c0d7a96"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["           video              d·ª± ƒëo√°n\n","0     video1.mp4  barbell biceps curl\n","1     video2.mp4          bench press\n","2     video3.mp4    chest fly machine\n","3     video4.mp4          tricep dips\n","4     video5.mp4  incline bench press\n","5     video6.mp4        lateral raise\n","6     video7.mp4        leg extension\n","7     video8.mp4           leg raises\n","8     video9.mp4         lat pulldown\n","9    video10.mp4        lateral raise\n","10   video11.mp4        leg extension\n","11   video12.mp4      tricep Pushdown\n","12   video13.mp4                plank\n","13   video14.mp4                squat\n","14   video15.mp4              push-up\n","15   video16.mp4    romanian deadlift\n","16  video17t.mp4        russian twist\n","17   video18.mp4         lat pulldown\n","18   video19.mp4                squat\n","19   video20.mp4  incline bench press\n","20   video21.mp4          tricep dips\n","21   video22.mp4      tricep Pushdown\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1O4N5Vbzv0rfkT81LQidPktX8RtoS5A40","timestamp":1709264520060}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}